# 矩阵填充方法对比实验报告

## 1. 引言

### 1.1 问题背景与意义
矩阵填充是推荐系统、图像处理和生物信息学等领域的核心问题。在MovieLens电影推荐任务中，用户-电影评分矩阵极度稀疏（密度约1.34%），如何从有限的观测中恢复完整矩阵具有重要理论和应用价值。

### 1.2 问题形式化
给定观测矩阵 $M \in \mathbb{R}^{n \times m}$，仅知道子集 $\Omega$上的元素 $M_{ij}, (i,j) \in \Omega$，基于低秩假设重建完整矩阵：
$$\min_X \text{Rank}(X) \quad \text{s.t.} \quad P_\Omega(X) = P_\Omega(M)$$

### 1.3 实验目标
- 实现核范数最小化（凸方法）
- 实现至少一种非凸低秩矩阵填充方法
- 基于5折交叉验证RMSE评估算法性能
- 对比分析凸与非凸方法的优劣

## 2. 理论分析

### 2.1 凸优化方法：核范数最小化

#### 2.1.1 数学形式

$$\min_X \frac{1}{2} \|P_\Omega(X) - P_\Omega(M)\|_F^2 + \lambda \|X\|_*$$

其中, 

$$\|X\|_* = \sum_{i=1}^{\min(m,n)} \sigma_i(X)$$

为核范数（奇异值之和）。

#### 2.1.2 Soft-Impute算法原理
迭代更新公式：
$$X_{k+1} = S_{\lambda \eta}\left(X_k - \eta \nabla f(X_k)\right)$$
其中：
- $S_{\tau}(Y) = U\Sigma_\tau V^T$为软阈值算子
- $\Sigma_\tau = \text{diag}(\max(\sigma_i - \tau, 0))$
- $\eta$为步长

**收敛性**：保证收敛到全局最优解，收敛速度 $O(1/k)$。

### 2.2 非凸优化方法

#### 2.2.1 交替最小化（Alternating Minimization）
矩阵分解形式: $X = UV^T$
优化问题：
$$\min_{U,V} \frac{1}{2} \sum_{(i,j)\in\Omega} (U_i V_j^T - M_{ij})^2 + \frac{\lambda}{2}(\|U\|_F^2 + \|V\|_F^2)$$

更新规则：
- 固定 $V$，更新 $U$: $U_i = \left(\sum_{j\in\Omega_i} v_j v_j^T + \lambda I\right)^{-1} \left(\sum_{j\in\Omega_i} M_{ij} v_j\right)$
- 固定 $U$，更新 $V$: $V_j = \left(\sum_{i\in\Omega_j} u_i u_i^T + \lambda I\right)^{-1} \left(\sum_{i\in\Omega_j} M_{ij} u_i\right)$

#### 2.2.2 梯度下降法
目标函数：
$$f(U,V) = \frac{1}{2} \|P_\Omega(UV^T) - P_\Omega(M)\|_F^2 + \frac{\lambda}{2}(\|U\|_F^2 + \|V\|_F^2)$$

梯度：
$$\nabla_U f = (P_\Omega(UV^T) - P_\Omega(M))V + \lambda U$$
$$\nabla_V f = (P_\Omega(UV^T) - P_\Omega(M))^T U + \lambda V$$

#### 2.2.3 带特殊正则化的矩阵补全（MC + Regularization）

**目标函数**：
$$\min_{U,V} \frac{1}{2} \sum_{(i,j)\in\Omega} (U_i V_j^T - M_{ij})^2 + \frac{\lambda_1}{2}(\|U\|_F^2 + \|V\|_F^2) + \lambda_2 Q(U)$$

**特殊正则化项**：

$$Q(U) = \sum_{i=1}^{n} \left(\max\left(\|U_i\|_2 - \alpha, 0\right)\right)^4 = \sum_{i=1}^{n} \left(\|U_i\|_2 - \alpha\right)_+^4$$

其中：
- $U_i$表示矩阵 $U$的第 $i$行（用户$i$的隐因子向量）
- $\|U_i\|_2$为 $\ell_2$范数
- $\alpha > 0$为阈值参数
- $(\cdot)_+ = \max(\cdot, 0)$为正部函数

**正则化项性质分析**：
1. **非凸非光滑性**：当 $\|U_i\|_2 < \alpha$时，正则化项为零；当 $\|U_i\|_2 > \alpha$时，正则化项为四次函数
2. **稀疏性促进**：鼓励用户因子的 $\ell_2$范数不超过 $\alpha$，防止某些用户的隐因子过大
3. **平衡效应**：避免少数用户主导推荐结果，提高模型的鲁棒性

**梯度计算**：

$$\nabla_{U_i} Q(U) = 
\begin{cases} 
4(\|U_i\|_2 - \alpha)^3 \cdot \frac{U_i}{\|U_i\|_2}, & \text{if } \|U_i\|_2 > \alpha \\
0, & \text{otherwise}
\end{cases}$$

**优化方法**：使用随机梯度下降，初始化采用随机正态分布。

## 3. 实验设置

### 3.1 数据集
- **数据集**：MovieLens 10M
- **用户数**：69,878
- **电影数**：10,677
- **评分总数**：10,000,054
- **矩阵密度**：约1.34%
- **评分范围**：1-5分

### 3.2 评估方法
- **评价指标**：均方根误差（RMSE）
- **验证方法**：5折交叉验证
- **矩阵秩**：rank = 10

### 3.3 对比方法
1. **Soft-Impute**：凸方法，核范数正则化
2. **Alternating Minimization**：非凸方法，交替最小化
3. **Gradient Descent (Spectral Init)**：非凸方法，谱初始化梯度下降
4. **MC + Regularization (Random Init)**：非凸方法，特殊正则化项

## 4. 实验结果与分析

### 4.1 性能对比

| 方法 | 平均测试RMSE | 标准差 | 性能排名 |
|------|-------------|--------|----------|
| Alternating Minimization | 0.8265 | ±0.0003 | 1 |
| Soft-Impute | 0.9170 | ±0.0033 | 2 |
| Gradient Descent (Spectral Init) | 2.1935 | ±0.0120 | 3 |
| MC + Regularization (Random Init) | 2.7270 | ±0.0005 | 4 |

### 4.2 可解释性分析

#### 4.2.1 交替最小化最优性能分析
1. **问题结构匹配**：MovieLens评分矩阵天然适合低秩分解
2. **算法稳定性**：每次子问题为凸二次规划，保证收敛
3. **初始化优势**：谱初始化接近真实解
4. **正则化效果**：适度的 $\ell_2$正则化防止过拟合
5. **计算效率**：复杂度 $O(r|\Omega|)$，适合稀疏矩阵

#### 4.2.2 Soft-Impute次优原因
1. **计算近似**：随机化SVD引入近似误差
2. **参数敏感**：正则化参数 $\lambda$选择困难
3. **内存限制**：处理大规模稠密矩阵内存消耗大
4. **收敛速度**：需要更多迭代达到高精度

#### 4.2.3 梯度下降方法性能差异
1. **学习率敏感**：固定学习率难以适应不同特征方向
2. **梯度问题**：稀疏观测导致梯度估计方差大
3. **局部极小**：非凸曲面存在多个局部极小点
4. **正则化不足**：正则化强度需要精细调整

### 4.3 统计显著性分析
基于5折交叉验证结果进行配对t检验：
- Alternating Minimization vs Soft-Impute: $p < 0.001$，差异显著
- Alternating Minimization vs Gradient Descent: $p < 0.0001$，差异极显著

## 5. 算法复杂度分析

### 5.1 时间复杂度对比
| 方法 | 每迭代复杂度 | 收敛速度 | 总复杂度 |
|------|-------------|----------|----------|
| Soft-Impute | $O(rnm)$ | $O(1/\epsilon)$ | $O(rnm/\epsilon)$ |
| 交替最小化 | $O(r\|\Omega\|)$ | 线性收敛 | $O(r\|\Omega\|\log(1/\epsilon))$ |
| 梯度下降 | $O(r\|\Omega\|)$ | $O(1/\epsilon)$ | $O(r\|\Omega\|/\epsilon)$ |

其中：
- $n,m$：矩阵维度
- $r$：矩阵秩
- $|\Omega|$：观测数
- $\epsilon$：目标精度

### 5.2 实际运行时间分析

#### 5.2.1 各方法训练时间汇总（5折平均值）

| 方法 | 平均训练时间（秒） | 标准差（秒） | 相对速度比（以AM为基准） |
|------|------------------|--------------|------------------------|
| Soft-Impute | 9,431.5 | 1,820.2 | 0.05× |
| Alternating Minimization | 525.9 | 18.3 | 1.00×（基准） |
| Gradient Descent (Spectral Init) | 2,871.8 | 570.2 | 0.18× |
| MC + Regularization (Random Init) | 187.3 | 27.3 | 2.81× |

#### 5.2.2 收敛性分析

![](收敛性分析12.png)

![](收敛性分析34.png)


### 5.3.1 空间复杂度对比
- **Soft-Impute**: $O(nm)$（稠密矩阵）
- **交替最小化**: $O(r(n+m))$（因子矩阵）
- **梯度下降**: $O(r(n+m))$（因子矩阵）

#### 5.3.2 实际内存挑战

1. **Soft-Impute内存瓶颈**：
   - 需要存储69878×10677的稠密矩阵
   - 实际运行时内存需求超过5GB（含中间变量）
   - 内存交换频繁，影响计算速度
   - 不适合更大规模数据

2. **非凸方法内存优势**：
   - 仅存储低维因子矩阵
   - 内存需求与观测数无关，仅与矩阵维度线性相关
   - 适合扩展到千万级用户/物品

3. **稀疏性利用**：
   - 非凸方法可充分利用评分矩阵稀疏性
   - 仅需存储观测位置信息
   - 计算复杂度与观测数线性相关
   
#### 5.3.3 内存使用与计算效率关系

| 方法 | 内存使用级别 | 计算效率 | 关系分析 |
|------|--------------|----------|----------|
| Soft-Impute | 极高 (GB级) | 极低 | 内存瓶颈导致效率低下 |
| 交替最小化 | 极低 (MB级) | 高 | 内存友好提升效率 |
| 梯度下降 | 极低 (MB级) | 中等 | 内存友好但算法问题影响效率 |
| MC+正则化 | 极低 (MB级) | 极高 | 内存友好但算法无效 |

# 实验改进
## 1.1 实验背景
基于第一次实验中发现的问题和局限性，我们对Soft-Impute算法进行了重大优化。第一次实验结果显示，Soft-Impute虽然理论保证强，但在大规模数据集上存在严重的计算效率和内存消耗问题。本次实验旨在通过算法优化，解决这些问题，提升Soft-Impute在实际应用中的实用性。

## 1.2 实验目标
1. 实现高效的Soft-Impute算法，解决内存和计算瓶颈
2. 在保持理论优势的同时提升实际性能
3. 验证优化后算法的实用性和可扩展性
4. 对比优化前后算法的性能差异

## 2. 优化方法与理论改进

### 2.1 第一次实验中的问题总结
| 问题类别 | 具体表现 | 影响程度 |
|----------|----------|----------|
| 计算效率 | 单折训练时间~2.6小时 | 严重 |
| 内存消耗 | 需要存储746M元素的稠密矩阵 | 严重 |
| 算法实现 | 随机化SVD实现不完善 | 中等 |
| 参数选择 | λ参数敏感，调参困难 | 中等 |

### 2.2 第二次实验的优化策略

#### 2.2.1 中心化处理（Mean-Centering）
**数学原理**：
$$\tilde{M}_{ij} = M_{ij} - (\mu + b_u(i) + b_i(j))$$
其中：
- $\mu$：全局平均评分
- $b_u(i)$：用户$i$的偏置项
- $b_i(j)$：物品$j$的偏置项

**优化效果**：
- 减少数据方差，提高数值稳定性
- 加速SVD收敛
- 降低矩阵的数值范围

#### 2.2.2 偏置项计算
用户偏置：
$$b_u(i) = \frac{\sum_{j\in\Omega_i} M_{ij}}{|\Omega_i|} - \mu$$
物品偏置：
$$b_i(j) = \frac{\sum_{i\in\Omega_j} M_{ij}}{|\Omega_j|} - \mu$$

#### 2.2.3 随机SVD优化
使用`sklearn.utils.extmath.randomized_svd`：
- 计算复杂度：$O(rk(n+m) + k^3)$，其中$k$为幂迭代次数
- 内存需求：仅需存储部分矩阵
- 支持大规模矩阵分解

#### 2.2.4 批量处理与内存优化
1. **增量计算**：避免存储完整的稠密中间矩阵
2. **批量预测**：将预测任务分批次处理，减少内存峰值
3. **数据类型优化**：使用float32替代float64，节省内存

### 2.3 改进后算法的数学模型
**优化问题**：
$$\min_{X_{\text{center}}} \frac{1}{2} \|P_\Omega(X_{\text{center}}) - P_\Omega(\tilde{M})\|_F^2 + \lambda \|X_{\text{center}}\|_*$$

**最终预测**：
$$\hat{M}_{ij} = \mu + b_u(i) + b_i(j) + X_{\text{center},ij}$$

**软阈值SVD更新**：
$$X_{\text{center}}^{(k+1)} = U^{(k)} \cdot \text{diag}(\max(\sigma_i^{(k)} - \lambda, 0)) \cdot V^{(k)T}$$

## 3. 实验设置

### 3.1 实验配置
| 参数 | 第一次实验 | 第二次实验 |
|------|------------|------------|
| 矩阵秩(r) | 固定为10 | 测试[10, 50, 100] |
| 正则化参数(λ) | 使用λ路径[0.5, ...] | 固定测试[60, 0.1] |
| SVD方法 | 自定义随机SVD | sklearn.randomized_svd |
| 中心化处理 | 无 | 有（均值+偏置） |
| 最大迭代次数 | 20 | 15 |
| 收敛阈值 | 1e-3 | 1e-4 |

### 3.2 评估指标
1. **测试RMSE**：预测评分与实际评分的均方根误差
2. **训练RMSE**：训练集上的均方根误差
3. **训练时间**：单折训练所需时间（秒）
4. **内存使用**：算法运行时的内存峰值

## 4. 实验结果与分析

### 4.1 性能对比（第一次vs第二次实验）

#### 4.1.1 Soft-Impute性能提升
| 指标 | 第一次实验 | 第二次实验 | 改进幅度 |
|------|------------|------------|----------|
| 测试RMSE | 0.9170 | 0.8168 (rank=100) | ↓10.9% |
| 训练时间/折 | 9,431.5秒 | 436.1秒 | ↑95.4% |
| 内存需求 | ~5.6GB | ~2.5GB | ↓55.4% |
| 收敛迭代 | 35+次 | 15次以内 | ↓57.1% |

#### 4.1.2 与交替最小化的对比
| 方法 | 测试RMSE | 训练时间/折 | 综合排名 |
|------|----------|-------------|----------|
| 第一次实验-交替最小化 | 0.8265 | 525.9秒 | 1 |
| 第二次实验-SoftImpute | 0.8168 | 436.1秒 | 1 |
| 改进效果 | ↓1.17% | ↓17.1% | 全面超越 |

### 4.2 参数敏感性分析

#### 6.4.2.1 矩阵秩的影响
| 矩阵秩 | 平均测试RMSE | 平均训练时间 | 效果评价 |
|--------|--------------|--------------|----------|
| 10 | 0.8290 | 318.3秒 | 基础效果 |
| 50 | 0.8181 | 347.6秒 | 显著提升 |
| 100 (λ=60) | 0.8168 | 436.1秒 | 最优效果 |
| 100 (λ=0.1) | 0.8515 | 454.6秒 | 过拟合 |

**发现**：
1. 随着秩增加，RMSE逐渐降低（10→50→100）
2. 秩从50增加到100时，RMSE仅改善0.16%，但时间增加25.5%
3. 秩100时，训练时间与效果达到较好平衡

#### 4.2.2 正则化参数的影响
对比λ=60和λ=0.1（秩均为100）：
- λ=60：RMSE=0.8168，训练RMSE=0.7294
- λ=0.1：RMSE=0.8515，训练RMSE=0.5658

**分析**：
1. λ=0.1时训练RMSE很低但测试RMSE很高，表明严重过拟合
2. λ=60的正则化强度适中，泛化性能好
3. 软阈值SVD中λ直接影响奇异值的保留数量

### 4.3 收敛特性分析

#### 4.3.1 收敛曲线观察
从提供的收敛曲线图（时间 vs 中心化MSE）分析：
- 曲线从~0.71快速下降至~0.64
- 收敛过程平滑稳定，无剧烈震荡
- 在300秒左右基本收敛，后期改进有限

#### 4.3.2 收敛速度量化
| 阶段 | 时间范围 | MSE变化 | 下降速度 |
|------|----------|----------|----------|
| 快速下降期 | 0-100秒 | 0.71→0.67 | 0.4 MSE/100s |
| 稳定收敛期 | 100-300秒 | 0.67→0.65 | 0.1 MSE/100s |
| 收敛后期 | 300-350秒 | 0.65→0.64 | 0.2 MSE/100s |

**特点**：
1. 早期收敛快，后期收敛慢
2. 总共15次迭代在350秒内完成
3. 比第一次实验（35+次迭代，>2小时）显著提升

### 4.4 稳定性分析

#### 4.4.1 各折结果一致性
| 配置 | 标准差 | 最大波动 | 稳定性评价 |
|------|--------|----------|------------|
| rank=10, λ=60 | ±0.00015 | 0.00036 | 极高 |
| rank=50, λ=60 | ±0.00020 | 0.00041 | 极高 |
| rank=100, λ=60 | ±0.00021 | 0.00049 | 极高 |
| rank=100, λ=0.1 | ±0.00018 | 0.00044 | 极高 |

**发现**：所有配置在各折间的标准差极小（<0.00025），表明算法稳定性极高。

#### 4.4.2 训练时间稳定性
| 配置 | 训练时间标准差 | 时间波动率 | 稳定性评价 |
|------|----------------|------------|------------|
| rank=10 | ±38.5秒 | 12.1% | 良好 |
| rank=50 | ±14.8秒 | 4.3% | 优秀 |
| rank=100 (λ=60) | ±15.5秒 | 3.6% | 优秀 |
| rank=100 (λ=0.1) | ±63.8秒 | 14.0% | 中等 |

### 4.5 计算效率分析

#### 4.5.1 时间分解估计
| 计算阶段 | 时间占比 | 优化关键 | 进一步优化空间 |
|----------|----------|----------|----------------|
| 中心化计算 | ~5% | 使用bincount加速 | 有限 |
| SVD分解 | ~70% | 随机SVD，幂迭代7次 | 调整幂迭代次数 |
| 矩阵重构 | ~15% | 部分矩阵乘法 | 使用BLAS优化 |
| 其他 | ~10% | Python开销 | 使用Cython |

#### 4.5.2 内存使用优化
| 数据结构 | 第一次实验 | 第二次实验 | 优化效果 |
|----------|------------|------------|----------|
| 稠密矩阵 | 2.98GB | 0GB | 完全消除 |
| 因子矩阵 | 0GB | ~800MB | 新增但可控 |
| 中间变量 | ~2.6GB | ~1.7GB | 减少34.6% |
| 总计 | ~5.6GB | ~2.5GB | 减少55.4% |

## 5. 深入分析与理论解释

### 5.1 为什么优化后效果显著提升

#### 5.1.1 中心化的理论优势
中心化处理相当于预训练了一个基线模型：
$$\text{基线预测} = \mu + b_u + b_i$$
这个基线模型已经能解释大部分评分变异，剩余部分由低秩矩阵捕获，这使得优化问题更容易求解。

#### 5.1.2 随机SVD的精度保证
随机SVD理论保证：对于秩$r$近似，误差满足：
$$\|A - \tilde{A}_r\|_F \leq (1+\epsilon)\|A - A_r\|_F$$
其中$A_r$为最优秩$r$近似。通过调整幂迭代次数，可以控制近似精度。

#### 5.1.3 软阈值的统计解释
软阈值操作：
$$\sigma_i^{\text{shrunk}} = \max(\sigma_i - \lambda, 0)$$
这等价于在奇异值上施加Laplace先验，具有贝叶斯解释，能有效防止过拟合。

### 5.2 过拟合现象分析
当λ=0.1时观察到的过拟合：
- **训练RMSE**：0.5658（非常低）
- **测试RMSE**：0.8515（明显更高）
- **原因**：λ太小导致保留过多奇异值，模型过于复杂

